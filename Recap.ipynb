{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recap:\n",
    "\n",
    "Recreating the autoencoder from the paper using the same shape\n",
    "- We wanted to return the model, encoder and decoder, and latent layer\n",
    "- Create layers in an encoder using sequential layer by creating a list that is pased to keras.sequential\n",
    "    - percentage layer transform\n",
    "    - dense layer 512 nodes with leakrelu activation function \n",
    "        - Which helps with overfitting and dead nodes. \n",
    "    - dropout layer \n",
    "        - The dropout layer is responsible for randomly skipping the neurons inside the neural network so that the overall odds of overfitting are reduced in an optimized manner.\n",
    "- Create layers in a decoder\n",
    "    - Inverse of creating the layers in the encoder\n",
    "        - except for the percentage layer transform\n",
    "\n",
    "Loss function issues\n",
    "- Added ReLU activation function in each layer\n",
    "- Adding braycurtis loss function with percentage during autoencoder compilation. Used custom loss function from authors.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
